<Doc xmlns:gs2="http://www.greenstone.org/gs2" file="HASHe01e.dir\doc.xml"  gs2:docOID="HASHe01ede834a2ebcd3234133" gs2:mode="add">

<Sec  gs2:docOID="HASHe01ede834a2ebcd3234133" gs2:mode="add">








&lt;div id=&quot;page1&quot;&gt;

 
</Sec>

<Sec  gs2:docOID="HASHe01ede834a2ebcd3234133.1" gs2:mode="add">


 
</Sec>

<Sec  gs2:docOID="HASHe01ede834a2ebcd3234133.1.1" gs2:mode="add">


&lt;div style=&quot;position: relative; height: 792px; width: 612px;&quot;&gt;&lt;style type=&quot;text/css&quot;&gt;

.txt { white-space:nowrap; }

.p1f0 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p1f1 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p1f2 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p1f3 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p1f4 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p1f5 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p1f6 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p1f7 { font-family:sans-serif; font-weight:normal; font-style:normal; }

&lt;/style&gt;

&lt;img class=&quot;background&quot; height=&quot;792&quot; id=&quot;background1&quot; src=&quot;_httpdocimg_/page1.png&quot; style=&quot;position:absolute; left:0px; top:0px;&quot; width=&quot;612&quot;&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:142px; top:164px;&quot;&gt;&lt;span class=&quot;p1f1&quot; style=&quot;font-size:15px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Learning an agent to walk using Reinforcement&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:275px; top:186px;&quot;&gt;&lt;span class=&quot;p1f1&quot; style=&quot;font-size:15px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Learning&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:213px; top:219px;&quot;&gt;&lt;span class=&quot;p1f2&quot; style=&quot;font-size:11px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Viktorija Mijalcheva, Ana Davcheva&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:266px; top:242px;&quot;&gt;&lt;span class=&quot;p1f2&quot; style=&quot;font-size:11px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;December 2021&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:285px; top:280px;&quot;&gt;&lt;span class=&quot;p1f3&quot; style=&quot;font-size:8px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Abstract&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:172px; top:296px;&quot;&gt;&lt;span class=&quot;p1f4&quot; style=&quot;font-size:8px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Reinforcement learning offers one of the most general frameworks to&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:307px;&quot;&gt;&lt;span class=&quot;p1f4&quot; style=&quot;font-size:8px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;take traditional robotics towards true autonomy and versatility. Deep Re-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:318px;&quot;&gt;&lt;span class=&quot;p1f4&quot; style=&quot;font-size:8px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;inforcement Learning methods have been successfully applied in many en-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:329px;&quot;&gt;&lt;span class=&quot;p1f4&quot; style=&quot;font-size:8px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;vironments and used instead of traditional, optimal and adaptive control&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:340px;&quot;&gt;&lt;span class=&quot;p1f4&quot; style=&quot;font-size:8px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;methods for some complex problems. However, applying deep reinforce-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:351px;&quot;&gt;&lt;span class=&quot;p1f4&quot; style=&quot;font-size:8px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;ment learning to partially observed environments is still a big challenge.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:361px;&quot;&gt;&lt;span class=&quot;p1f4&quot; style=&quot;font-size:8px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;When an agent is not informed well of the environment, it must recover&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:372px;&quot;&gt;&lt;span class=&quot;p1f4&quot; style=&quot;font-size:8px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;information from the past observations. In this paper are analysed differ-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:383px;&quot;&gt;&lt;span class=&quot;p1f4&quot; style=&quot;font-size:8px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;ent approaches for training a robot to walk in the Gym[12] environment&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:394px;&quot;&gt;&lt;span class=&quot;p1f4&quot; style=&quot;font-size:8px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;- Bipedal Walker [13]. The main focus will be to implement and compare&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:405px;&quot;&gt;&lt;span class=&quot;p1f4&quot; style=&quot;font-size:8px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;the results from various algorithms such as: Proximal Policy Optimization&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:416px;&quot;&gt;&lt;span class=&quot;p1f4&quot; style=&quot;font-size:8px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(PPO2)[9], Augmented random search (ARS)[17] and Deep Q-Network&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:427px;&quot;&gt;&lt;span class=&quot;p1f4&quot; style=&quot;font-size:8px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(DQN)[8].&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:456px;&quot;&gt;&lt;span class=&quot;p1f5&quot; style=&quot;font-size:14px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;1 Introduction&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:481px;&quot;&gt;&lt;span class=&quot;p1f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Embodied cognition is the theory that an organism’s cognitive abilities is shaped&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:493px;&quot;&gt;&lt;span class=&quot;p1f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;by its body. It is even argued that an agent’s cognition extends beyond its brain,&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:505px;&quot;&gt;&lt;span class=&quot;p1f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;and is strongly influenced by aspects of its body and also the experiences from&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:517px;&quot;&gt;&lt;span class=&quot;p1f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;its various sensorimotor functions. Evolution plays a vital role in shaping an&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:529px;&quot;&gt;&lt;span class=&quot;p1f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;organism’s body to adapt to its environment; the brain and its ability to learn&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:541px;&quot;&gt;&lt;span class=&quot;p1f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is only one of many body components that is co-evolved together.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:553px;&quot;&gt;&lt;span class=&quot;p1f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;While evolution shapes the overall structure of the body of a particular&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:565px;&quot;&gt;&lt;span class=&quot;p1f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;species, an organism can also change and adapt its body to its environment&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:577px;&quot;&gt;&lt;span class=&quot;p1f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;during its life. For instance, professional athletes spend their lives body training&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:589px;&quot;&gt;&lt;span class=&quot;p1f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;while also improving specific mental skills required to master a particular sport&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:601px;&quot;&gt;&lt;span class=&quot;p1f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[20]. In everyday life, regular exercise not only strengthens the body but also&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:613px;&quot;&gt;&lt;span class=&quot;p1f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;improves mental conditions [4]. We not only learn and improve our skills and&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:625px;&quot;&gt;&lt;span class=&quot;p1f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;abilities during our lives, but also learn to shape our bodies for the lives we&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:637px;&quot;&gt;&lt;span class=&quot;p1f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;want to live.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:649px;&quot;&gt;&lt;span class=&quot;p1f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Reinforcement Learning is one of the three main machine learning paradigms&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:661px;&quot;&gt;&lt;span class=&quot;p1f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;along with Supervised and Unsupervised Learning. It is the closest kind of&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:303px; top:695px;&quot;&gt;&lt;span class=&quot;p1f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;1&lt;/span&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;



&lt;div id=&quot;page2&quot;&gt;

 
</Sec>

<Sec  gs2:docOID="HASHe01ede834a2ebcd3234133.1.2" gs2:mode="add">


&lt;div style=&quot;position: relative; height: 792px; width: 612px;&quot;&gt;&lt;style type=&quot;text/css&quot;&gt;

.txt { white-space:nowrap; }

.p2f0 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p2f1 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p2f2 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p2f3 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p2f4 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p2f5 { font-family:sans-serif; font-weight:normal; font-style:normal; }

&lt;/style&gt;

&lt;img class=&quot;background&quot; height=&quot;792&quot; id=&quot;background2&quot; src=&quot;_httpdocimg_/page2.png&quot; style=&quot;position:absolute; left:0px; top:0px;&quot; width=&quot;612&quot;&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:127px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;learning demonstrated by humans and animals since it is grounded by biologi-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:139px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;cal learning systems. It is based on maximizing cumulative reward over time to&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:151px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;make agent learn how to act in an environment. In many reinforcement learn-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:163px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;ing tasks, the goal is to learn a policy to manipulate an agent, whose design&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:175px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is fixed, to maximize some notion of cumulative reward. Continuous action&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:187px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;spaces impose a serious challenge for reinforcement learning agents. While sev-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:199px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;eral off-policy reinforcement learning algorithms provide a universal solution to&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:211px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;continuous control problems, the real challenge lies in the fact that different ac-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:223px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;tuators feature different response functions due to wear and tear (in mechanical&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:235px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;systems) and fatigue (in biomechanical systems)[7].&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:247px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Formally, Reinforcement Learning is learning a policy function (strategy of&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:259px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;the agent) &lt;/span&gt;&lt;span class=&quot;p2f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;π &lt;/span&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;: &lt;/span&gt;&lt;span class=&quot;p2f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;S &lt;/span&gt;&lt;span class=&quot;p2f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;p2f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;A &lt;/span&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;which maps inputs (states) s &lt;/span&gt;&lt;span class=&quot;p2f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;ϵ &lt;/span&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;S to outputs (actions) a &lt;/span&gt;&lt;span class=&quot;p2f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;ϵ &lt;/span&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;A.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:269px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Learning is achieved by maximization of the value function &lt;/span&gt;&lt;span class=&quot;p2f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;V &lt;/span&gt;&lt;span class=&quot;p2f4&quot; style=&quot;font-size:6px;vertical-align:super;color:rgba(0,0,0,1);&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;p2f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;) (cumulative&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:283px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;reward) for all possible states, which depends on policy &lt;/span&gt;&lt;span class=&quot;p2f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;. In this sense, it&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:295px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is similar to unsupervised learning. However, the difference is that the value&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:305px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;function &lt;/span&gt;&lt;span class=&quot;p2f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;V &lt;/span&gt;&lt;span class=&quot;p2f4&quot; style=&quot;font-size:6px;vertical-align:super;color:rgba(0,0,0,1);&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;p2f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;) is not defined exactly unlike unsupervised learning setting. It is&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:319px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;also learned by interacting with the environment by taking all possible actions&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:331px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;in all possible states.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:343px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;In spite of tremendous leaps in computing power as well as major advances in&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:355px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;the development of materials, motors, power supplies and sensors, there is still&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:367px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;lack of the ability to create a humanoid robotic system that even comes close to&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:379px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;a similar level of robustness, versatility and adaptability as biological systems.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:390px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Among the key missing elements is the ability to create control systems that can&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:402px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;deal with a large movement repertoire, variable speeds, constraints and most&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:414px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;importantly, uncertainty in the real-world environment in a fast, reactive man-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:426px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;ner. Most baseline tasks in the RL literature test an algorithm’s ability to learn&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:438px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;a policy to control the actions of an agent, with a predetermined body design,&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:450px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;to accomplish a given task inside an environment. This makes it highly suitable&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:462px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;for systems which are difficult to control using conventional control method-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:474px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;ologies, such as walking robots. Traditionally, RL has only been applicable to&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:486px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;problems with low dimensional state space, but use of Deep Neural Networks as&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:498px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;function approximators with RL has shown impressive results for control of high&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:510px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;dimensional systems. This approach is known as Deep Reinforcement Learning&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:522px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(DRL). A major drawback of DRL algorithms is that they generally require a&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:534px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;large number of samples and training time, which becomes a challenge when&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:546px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;working with real robots. Therefore, most applications of DRL methods have&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:558px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;been limited to simulation platforms.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:570px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;In this thesis, Bipedal Locomotion is investigated through BipedalWalker-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:582px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Hardcorev3 [12] environment of open source GYM library by DRL[2]. In the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:594px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;OpenAI Gym’s BipedalWalker-v3 environment, the reward system is defined by&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:606px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;incrementally gaining 300 points if the finish line is reached, losing 100 points&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:618px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;if robot falls over, and gradually losing points based on the amount of control&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:630px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;effort exerted from the motors. Solving the problem is defined by collecting&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:642px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;over 300 reward points for 100 consecutive episodes. Various algorithms such as:&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:653px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Proximal Policy Optimization (PPO2)[9], Augmented random search (ARS)[17]&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:665px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;and Deep Q-Network (DQN)[8] are tried in order to get the agent walking.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:303px; top:695px;&quot;&gt;&lt;span class=&quot;p2f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;2&lt;/span&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;



&lt;div id=&quot;page3&quot;&gt;

 
</Sec>

<Sec  gs2:docOID="HASHe01ede834a2ebcd3234133.1.3" gs2:mode="add">


&lt;div style=&quot;position: relative; height: 792px; width: 612px;&quot;&gt;&lt;style type=&quot;text/css&quot;&gt;

.txt { white-space:nowrap; }

.p3f0 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p3f1 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p3f2 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p3f3 { font-family:sans-serif; font-weight:normal; font-style:normal; }

&lt;/style&gt;

&lt;img class=&quot;background&quot; height=&quot;792&quot; id=&quot;background3&quot; src=&quot;_httpdocimg_/page3.png&quot; style=&quot;position:absolute; left:0px; top:0px;&quot; width=&quot;612&quot;&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:127px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Furthermore, these algorithms will be deeply examined and compared with their&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:139px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;outcomes on the agent training.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:169px;&quot;&gt;&lt;span class=&quot;p3f2&quot; style=&quot;font-size:14px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;2 Related work&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:194px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Bipedal walking is one of the most challenging and intriguing controls problem&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:206px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;in recent years. It poses a variety of challenges which make it difficult to apply&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:218px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;classical control methodologies to it.In [15], Rastogi explained the four main rea-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:230px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;sons which makes these environments difficult, such as non-linear and changing&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:242px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;dynamics, multi-variable system and uncertainty in the model. Continuous ac-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:254px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;tion spaces impose a serious challenge for reinforcement learning agents. While&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:266px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;several off-policy reinforcement learning algorithms provide a universal solution&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:278px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;to continuous control problems, the real challenge lies in the fact that different&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:290px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;actuators feature different response functions due to wear and tear (in mechan-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:302px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;ical systems) and fatigue (in biomechanical systems). In [7], there is a soultion&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:314px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;proposed enhancing the actor-critic reinforcement learning agents by parame-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:326px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;terising the final layer in the actor network. This layer produces the actions to&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:338px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;accommodate the behaviour discrepancy of different actuators under different&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:350px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;load conditions during interaction with the environment. To achieve this, the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:362px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;actor is trained to learn the tuning parameter controlling the activation layer&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:374px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(e.g., Tanh and Sigmoid). The learned parameters are then used to create tai-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:385px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;lored activation functions for each actuator. Here, Deep Deterministic Policy&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:397px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Gradient (DDPG)[18] is used as a method in solving continuous state problems&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:409px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;and its use is shown as a helpful tool for dealing with this issue. All of the mod-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:421px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;els tried in this paper are also used in [13]. Later, it will be made a comparison&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:433px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;between the both papers. The results in this paper are not enough to conclude&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:445px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;on a superior neural network for all RL problems, because there are other factors&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:457px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;such as DRL algorithm, number of episodes, network size etc. However, net-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:469px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;works are designed to have similar sizes and a good model requires to converge&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:481px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;in less episodes. In addition, it is possible to conclude that Transformers can&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:493px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;be an option for partially observed RL problems. Today, all subfields of Deep&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:505px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Learning suffers from lack of analytical methods to design neural networks. It&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:517px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is mostly based on mathematical and logical intuition. Until such methods are&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:529px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;developed, it seems that we need to try out several neural networks to get bet-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:541px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;ter models, which is the case in our work. In [1], a comparison between the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:553px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;performance of two well known reinforcement learning (RL) algorithms is pre-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:565px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;sented. CACLA (Continuous Actor-Critic Learning Automaton)[21] and SPG&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:577px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(Sampled Policy Gradient)[22] are two RL algorithms designed to work with&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:589px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;continuous input and action spaces. In [1], CALCA performs better than SPG,&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:601px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;finishing the line around the episode 1600-1700. [14] is actually a book that has&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:613px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;all of the explanation of the joints, ankles, body of the bipedal robot. Bipeds&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:625px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;are multi-input, multi-output systems that are both continuous and discrete.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:637px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;While in single support, the system operates in a continuous fashion, as soon as&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:648px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;the support leg switches, there is discreteness, as well. There have been robots&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:660px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;such that their control is based on their certain joints and/or certain points on&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:303px; top:695px;&quot;&gt;&lt;span class=&quot;p3f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;3&lt;/span&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;



&lt;div id=&quot;page4&quot;&gt;

 
</Sec>

<Sec  gs2:docOID="HASHe01ede834a2ebcd3234133.1.4" gs2:mode="add">


&lt;div style=&quot;position: relative; height: 792px; width: 612px;&quot;&gt;&lt;style type=&quot;text/css&quot;&gt;

.txt { white-space:nowrap; }

.p4f0 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p4f1 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p4f2 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p4f3 { font-family:sans-serif; font-weight:normal; font-style:normal; }

&lt;/style&gt;

&lt;img class=&quot;background&quot; height=&quot;792&quot; id=&quot;background4&quot; src=&quot;_httpdocimg_/page4.png&quot; style=&quot;position:absolute; left:0px; top:0px;&quot; width=&quot;612&quot;&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:127px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;their structure track pre-specified trajectories. One of the advantages to this&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:139px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;approach is that the controller is relatively simple since all the trajectories are&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:151px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;known, but if there is a slight change in the shape of the robot or the terrain on&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:163px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;which the robot walks, the controller may not work any longer and it will usu-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:175px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;ally require supplemental control in addition to trajectory tracking. The major&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:187px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;problem we are currently dealing with is the noisiness of the velocity signals,&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:199px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;which limits us to use low damping parameters and makes the roll control more&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:211px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;challenging. A powerful filter can be quite helpful. Robustness measurement&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:223px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;of a biped control algorithm can be carried out in more extensive ways such as&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:235px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;applying different forces in different directions at different times while the robot&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:247px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is in different states. In [5] is shown that allowing a simple population-based&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:259px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;policy gradient method to learn not only the policy, but also a small set of&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:271px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;parameters describing the environment, such as its body, offers many benefits.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:283px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;By allowing the agent’s body to adapt to its task within some constraints, it&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:295px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;can learn policies that are not only better for its task, but also learn them more&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:307px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;quickly.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:337px;&quot;&gt;&lt;span class=&quot;p4f2&quot; style=&quot;font-size:14px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;3 Methods&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:362px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;The reinforcement learning environment poses a variety of obstacles that need&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:374px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;to be addressed and potentially to make trade-offs among them. In RL, an agent&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:385px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is supposed to maximize rewards (exploitation of knowledge) by observing the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:397px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;environment (exploration of environment). This gives rise to the exploration-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:409px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;exploitation dilemma which is an inevitable trade-off. Exploration means taking&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:421px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;a range of acts to benefit from the consequences, which typically results in low&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:433px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;immediate rewards but high rewards for the future.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:445px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;The first idea to work with a robot movement came from NeurIPS: AI for&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:457px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;prosthetics. In this competition, the task is developing a controller to enable a&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:469px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;physiologically based human model with a prosthetic leg to walk in requested&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:481px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;directions with varying speeds. The provided material is a human musculoskele-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:493px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;tal model and a physics-based simulation environment where you can synthesize&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:505px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;physically and physiologically accurate motion. Recent advancements in mate-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:517px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;rial science and device technology have increased interest in creating prosthetics&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:529px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;for improving human movement. Designing these devices, however, is difficult as&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:541px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;it is costly and time-consuming to iterate through many designs. This is further&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:553px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;complicated by the large variability in response among many individuals. One&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:565px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;key reason for this is that our understanding of the interactions between humans&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:577px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;and prostheses is not well-understood, which limits our ability to predict how a&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:589px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;human will adapt his or her movement. Physics-based, biomechanical simula-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:601px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;tions are well-positioned to advance this field as it allows for many experiments&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:613px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;to be run at low cost. Recent developments in using reinforcement learning tech-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:625px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;niques to train realistic, biomechanical models will be key to better understand&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:637px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;the human-prosthesis interaction, which will help to accelerate development of&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:648px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;this field. We tried all the similar environments here, but in the end they were&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:660px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;closed because the competitions were over, but in the continuation it will be&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:303px; top:695px;&quot;&gt;&lt;span class=&quot;p4f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;4&lt;/span&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;



&lt;div id=&quot;page5&quot;&gt;

 
</Sec>

<Sec  gs2:docOID="HASHe01ede834a2ebcd3234133.1.5" gs2:mode="add">


&lt;div style=&quot;position: relative; height: 792px; width: 612px;&quot;&gt;&lt;style type=&quot;text/css&quot;&gt;

.txt { white-space:nowrap; }

.p5f0 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p5f1 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p5f2 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p5f3 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p5f4 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p5f5 { font-family:sans-serif; font-weight:normal; font-style:normal; }

&lt;/style&gt;

&lt;img class=&quot;background&quot; height=&quot;792&quot; id=&quot;background5&quot; src=&quot;_httpdocimg_/page5.png&quot; style=&quot;position:absolute; left:0px; top:0px;&quot; width=&quot;612&quot;&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:127px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;explained what we researched and what we tried.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:139px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Formally, we tried to build a function a: S&lt;/span&gt;&lt;span class=&quot;p5f2&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;→&lt;/span&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;A from the state space S to the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:151px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;action space A. Each element s of the state space is represented as a dictionary&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:163px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;structure that includes current positions, velocities, accelerations of joints and&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:175px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;body parts, muscles activity, etc. The action space represents muscle activa-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:187px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;tions. The objective was to find such a function that reward throughout the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:199px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;episode is maximized.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:211px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;For training the agent, we tried the DDPG[18] algorithm from baselines[6]&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:223px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;source code. We imported the Actor and Critic and created the models. Then&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:235px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;for the training we used the train environment (imported from baselines.ddpg.training):&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:256px;&quot;&gt;&lt;span class=&quot;p5f2&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;training.train(env=env, evalenv=evalenv, paramnoise=paramnoise, action-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:269px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;noise=actionnoise, actor=actor, critic=critic, memory=memory)&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:291px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;We also tried to train RL with Deep Learning and made some neural networks,&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:303px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;but the environment was very complicated and required too much processing&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:315px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;power. Then we did a research of the solutions from the competition, where&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:327px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;we also found that the most used algorithms are DDPG[18] and PPO[9]. All&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:339px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;of the participants used servers on which they trained the data and also in&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:351px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;the early phase of training, participants reduced the search space or modified&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:363px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;the environment to speed up exploration using frameskip, exploration noise,&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:375px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;etc.. A common statistical technique for increasing the accuracy of models is to&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:386px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;output a weighted sum of multiple predictions. This technique also applies to&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:398px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;policies in reinforcement learning, and many teams used some variation of this&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:410px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;approach: ensemble of different checkpoints of models, training multiple agents&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:422px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;simultaneously, or training agents with different seeds. For this environment we&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:434px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;lost about 3 months of research, of which 1 month was needed to be able to set&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:446px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;the environment, trying on all operating systems. Then we waited a long time&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:458px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;to train ordinary models that lasted a long time and were often unsuccessful&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:470px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;due to insufficient memory. So from here, and from talking to other students&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:482px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;who are interested in this field, we searched for similar environments where we&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:494px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;can use the models we have created.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:506px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;The most similar environment that we found and which will be further used&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:518px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is the OpenAI Gym[12] enviornment called BipedalWalker-v3[13]. The Bipedal&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:530px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Walker series of environments is based on the Box2D [3] physics engine. The&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:542px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;environment provides a model of a five-link bipedal robot, depicted in Figure 1.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:554px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;The robot state is a vector with 24 elements: &lt;/span&gt;&lt;span class=&quot;p5f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;θ, &lt;/span&gt;&lt;span class=&quot;p5f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;·&lt;/span&gt;&lt;span class=&quot;p5f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;x, &lt;/span&gt;&lt;span class=&quot;p5f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;·&lt;/span&gt;&lt;span class=&quot;p5f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;y, ω &lt;/span&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;of the hull center of&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:566px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;mass (white), &lt;/span&gt;&lt;span class=&quot;p5f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;θ, ω &lt;/span&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;of each joint (two green, two orange), contacts with the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:578px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;ground (red), and 10 lidar scans of the ground (red line). The action space,&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:590px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;then, is a four element vector representing the torque command to the motor&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:602px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;at each of the four joints. This robot has to navigate through the environment,&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:614px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;which is a randomly seeded ground with an uneven surface. The lidar scans&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:626px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;can help detect this unevenness, though they are most useful in the hardcore&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:638px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;version of the environment where obstacles and stairs are present. To find the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:650px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;optimal policy, which maps the robot states to the robot actions yielding the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:661px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;highest reward, various algorithms are tried such as: PPO2, ARS and DQN.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:303px; top:695px;&quot;&gt;&lt;span class=&quot;p5f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;5&lt;/span&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;



&lt;div id=&quot;page6&quot;&gt;

 
</Sec>

<Sec  gs2:docOID="HASHe01ede834a2ebcd3234133.1.6" gs2:mode="add">


&lt;div style=&quot;position: relative; height: 792px; width: 612px;&quot;&gt;&lt;style type=&quot;text/css&quot;&gt;

.txt { white-space:nowrap; }

.p6f0 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p6f1 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p6f2 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p6f3 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p6f4 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p6f5 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p6f6 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p6f7 { font-family:sans-serif; font-weight:normal; font-style:normal; }

&lt;/style&gt;

&lt;img class=&quot;background&quot; height=&quot;792&quot; id=&quot;background6&quot; src=&quot;_httpdocimg_/page6.png&quot; style=&quot;position:absolute; left:0px; top:0px;&quot; width=&quot;612&quot;&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:333px;&quot;&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Figure 1: The robot in the BipedalWalker-v3 environment. Red flag indicates&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:345px;&quot;&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;starting position.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:370px;&quot;&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;The robot’s stimulus/environment is a grassy field. The robot’s decision&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:382px;&quot;&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;will be how to align its legs to stand up and walk. Depending on the robot’s&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:394px;&quot;&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;decision, the outcome will either be a reward (a few points for walking forward)&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:406px;&quot;&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;or a punishment (-100 points for falling down). The robot knows:&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:423px;&quot;&gt;&lt;span class=&quot;p6f4&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;How fast it’s going&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:442px;&quot;&gt;&lt;span class=&quot;p6f4&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;How its feet touch the ground&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:461px;&quot;&gt;&lt;span class=&quot;p6f4&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;How far away the ground is from its head (using a sensor of the kind found&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:473px;&quot;&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;in self-driving cars)&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:491px;&quot;&gt;&lt;span class=&quot;p6f4&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Whether it’s been rewarded or punished for its actions&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:510px;&quot;&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;The first algorithm that will be tried and explored is Augmented Random&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:522px;&quot;&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Search[17]. ARS explores the policy parameter space, as opposed to the action&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:534px;&quot;&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;and state space, by taking a set of parameter samples with zero mean Gaussian&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:546px;&quot;&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;noise and executing rollouts for each of those samples. The parameters of the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:558px;&quot;&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;model &lt;/span&gt;&lt;span class=&quot;p6f5&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;θ &lt;/span&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;take the form of an &lt;/span&gt;&lt;span class=&quot;p6f5&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;n &lt;/span&gt;&lt;span class=&quot;p6f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;× &lt;/span&gt;&lt;span class=&quot;p6f5&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;m &lt;/span&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;matrix where n is the number of action&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:570px;&quot;&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;variables and m is the number of state variables. A state observation is matrix&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:582px;&quot;&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;multiplied to produce a desired action to take. In ARS, several parameter&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:594px;&quot;&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;noise matrices &lt;/span&gt;&lt;span class=&quot;p6f5&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;δ &lt;/span&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;are sampled, resulting in &lt;/span&gt;&lt;span class=&quot;p6f5&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;θ &lt;/span&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;+ &lt;/span&gt;&lt;span class=&quot;p6f5&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;vδ &lt;/span&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;and &lt;/span&gt;&lt;span class=&quot;p6f5&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;θ &lt;/span&gt;&lt;span class=&quot;p6f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;− &lt;/span&gt;&lt;span class=&quot;p6f5&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;vδ &lt;/span&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;pairs where &lt;/span&gt;&lt;span class=&quot;p6f5&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;θ &lt;/span&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:606px;&quot;&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;the current parameterization, &lt;/span&gt;&lt;span class=&quot;p6f5&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;v &lt;/span&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is a step size scalar that determines how much&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:618px;&quot;&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;noise is introduced to the model, and &lt;/span&gt;&lt;span class=&quot;p6f5&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;δ &lt;/span&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;a random noise matrix of the same&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:630px;&quot;&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;dimensions as &lt;/span&gt;&lt;span class=&quot;p6f5&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;θ&lt;/span&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;. If in total there are h number of random noise matrices &lt;/span&gt;&lt;span class=&quot;p6f5&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;δ &lt;/span&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;then&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:642px;&quot;&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;there are 2h matrices, each of which are used to conduct rollouts. These noisy&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:653px;&quot;&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;parameterizations all lie within a hypersphere of a specified radius around the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:665px;&quot;&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;current parameterization &lt;/span&gt;&lt;span class=&quot;p6f5&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;θ&lt;/span&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:303px; top:695px;&quot;&gt;&lt;span class=&quot;p6f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;6&lt;/span&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;



&lt;div id=&quot;page7&quot;&gt;

 
</Sec>

<Sec  gs2:docOID="HASHe01ede834a2ebcd3234133.1.7" gs2:mode="add">


&lt;div style=&quot;position: relative; height: 792px; width: 612px;&quot;&gt;&lt;style type=&quot;text/css&quot;&gt;

.txt { white-space:nowrap; }

.p7f0 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p7f1 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p7f2 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p7f3 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p7f4 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p7f5 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p7f6 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p7f7 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p7f8 { font-family:monospace; font-weight:normal; font-style:normal; }

.p7f9 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p7f10 { font-family:sans-serif; font-weight:normal; font-style:normal; }

&lt;/style&gt;

&lt;img class=&quot;background&quot; height=&quot;792&quot; id=&quot;background7&quot; src=&quot;_httpdocimg_/page7.png&quot; style=&quot;position:absolute; left:0px; top:0px;&quot; width=&quot;612&quot;&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:127px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Rewards for each rollout are collected, and their standard deviation &lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;ω &lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:139px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;computed and used to normalize the final update step. The &lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;hvδ &lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;matrices&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:151px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;are then sorted in descending order of maximum reward using the criteria&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:163px;&quot;&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;θ&lt;/span&gt;&lt;span class=&quot;p7f4&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;vδ&lt;/span&gt;&lt;span class=&quot;p7f5&quot; style=&quot;font-size:4px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;i &lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, r&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;θ&lt;/span&gt;&lt;span class=&quot;p7f6&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;−&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;vδ&lt;/span&gt;&lt;span class=&quot;p7f5&quot; style=&quot;font-size:4px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;i &lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;) for each parameterization pair &lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, where &lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;θ&lt;/span&gt;&lt;span class=&quot;p7f4&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;vδ&lt;/span&gt;&lt;span class=&quot;p7f5&quot; style=&quot;font-size:4px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;i &lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;denotes the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:175px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;reward received from the rollout using a model parameterization &lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;θ &lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;+ &lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;vδ&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;i &lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, and&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:187px;&quot;&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;θ&lt;/span&gt;&lt;span class=&quot;p7f6&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;−&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;vδ&lt;/span&gt;&lt;span class=&quot;p7f5&quot; style=&quot;font-size:4px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;i &lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;denotes the reward received from the rollout using a model parameteri-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:199px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;zation &lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;θ &lt;/span&gt;&lt;span class=&quot;p7f7&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;− &lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;vδ&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;. The top &lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;m &lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;pairs are then used to update &lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;θ &lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;using the following&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:211px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;update rule:&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:265px; top:238px;&quot;&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;α&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:super;color:rgba(0,0,0,1);&quot;&gt;m&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:222px; top:248px;&quot;&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;θ&lt;/span&gt;&lt;span class=&quot;p7f7&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;←&lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;θ&lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;+&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:261px; top:255px;&quot;&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;mσ&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:293px; top:248px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;θ&lt;/span&gt;&lt;span class=&quot;p7f4&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;vδ&lt;/span&gt;&lt;span class=&quot;p7f5&quot; style=&quot;font-size:4px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;i &lt;/span&gt;&lt;span class=&quot;p7f7&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;− &lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;θ&lt;/span&gt;&lt;span class=&quot;p7f6&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;−&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;vδ&lt;/span&gt;&lt;span class=&quot;p7f5&quot; style=&quot;font-size:4px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;i &lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;) &lt;/span&gt;&lt;span class=&quot;p7f7&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;∗ &lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;δ&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;]&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:279px; top:262px;&quot;&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p7f4&quot; style=&quot;font-size:6px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;=1&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:279px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;where &lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;α &lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;represents the learning rate.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:291px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;As the agent learns and the average reward size increases, the summation&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:303px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;over &lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;θ&lt;/span&gt;&lt;span class=&quot;p7f4&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;vδ&lt;/span&gt;&lt;span class=&quot;p7f5&quot; style=&quot;font-size:4px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;i &lt;/span&gt;&lt;span class=&quot;p7f7&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;− &lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;θ&lt;/span&gt;&lt;span class=&quot;p7f6&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;−&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;vδ&lt;/span&gt;&lt;span class=&quot;p7f5&quot; style=&quot;font-size:4px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;i &lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;will increase as well, resulting in proportionately larger step&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:315px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;sizes as the policy improves overtime. To mitigate this inconsistency in step&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:327px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;size, the learning rate &lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;α &lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is normalized by the standard deviation of the rollout&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:339px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;rewards &lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;ω&lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;. This normalization is the primary difference between basic random&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:351px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;search (BRS) and augmented random search (ARS).&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:363px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;The next algorithm that will be tried to learn the agent to walk is proxi-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:375px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;mal policy optimization version 2 called PPO2 [9]. Policy gradient methods are&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:387px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;fundamental to recent breakthroughs in using deep neural networks for control,&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:399px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;from video games, to 3D locomotion, to Go. With supervised learning, cost&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:411px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;function can be easily implemented, gradient descent can be additionally run&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:423px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;on it, and excellent results will be the outcome with relatively little hyperpa-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:435px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;rameter tuning. Proximal policy optimization[9] strikes a balance between ease&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:446px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;of implementation, sample complexity, and ease of tuning, trying to compute&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:458px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;an update at each step that minimizes the cost function while ensuring the de-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:470px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;viation from the previous policy is relatively small. The variant of PPO uses a&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:482px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;novel objective function not typically found in other algorithms:&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:186px; top:502px;&quot;&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:super;color:rgba(0,0,0,1);&quot;&gt;CLIP &lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;θ&lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;) =Eˆ&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;θ&lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;)Aˆ &lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, clip&lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;θ&lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, &lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;1 &lt;/span&gt;&lt;span class=&quot;p7f7&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;− &lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;ε, &lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;1 + &lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;ε&lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;)Aˆ &lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;)]&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:525px;&quot;&gt;&lt;span class=&quot;p7f9&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;θ &lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is the policy parameter&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:544px;&quot;&gt;&lt;span class=&quot;p7f9&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Eˆ&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;t &lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;denotes the empirical expectation over timestamps&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:565px;&quot;&gt;&lt;span class=&quot;p7f9&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;t &lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is the ratio of the probability under the new and old policies, respectively&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:583px;&quot;&gt;&lt;span class=&quot;p7f9&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Aˆ &lt;/span&gt;&lt;span class=&quot;p7f3&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;t &lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is the estimated advantage at time t&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:605px;&quot;&gt;&lt;span class=&quot;p7f9&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p7f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;ε &lt;/span&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is a hyperparameter, usually 0.1 or 0.2&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:628px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;This objective implements a way to do a Trust Region update which is&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:640px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;compatible with Stochastic Gradient Descent, and simplifies the algorithm by&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:652px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;removing the KL penalty and need to make adaptive updates. In tests, this&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:664px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;algorithm has displayed the best performance on continuous control tasks. This&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:303px; top:695px;&quot;&gt;&lt;span class=&quot;p7f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;7&lt;/span&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;



&lt;div id=&quot;page8&quot;&gt;

 
</Sec>

<Sec  gs2:docOID="HASHe01ede834a2ebcd3234133.1.8" gs2:mode="add">


&lt;div style=&quot;position: relative; height: 792px; width: 612px;&quot;&gt;&lt;style type=&quot;text/css&quot;&gt;

.txt { white-space:nowrap; }

.p8f0 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p8f1 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p8f2 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p8f3 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p8f4 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p8f5 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p8f6 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p8f7 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p8f8 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p8f9 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p8f10 { font-family:monospace; font-weight:normal; font-style:normal; }

.p8f11 { font-family:sans-serif; font-weight:normal; font-style:normal; }

&lt;/style&gt;

&lt;img class=&quot;background&quot; height=&quot;792&quot; id=&quot;background8&quot; src=&quot;_httpdocimg_/page8.png&quot; style=&quot;position:absolute; left:0px; top:0px;&quot; width=&quot;612&quot;&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:127px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;paper uses the PPO2 implementation by baselines[6]. The release of baselines&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:139px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;includes scalable, parallel implementations of PPO, PPO2 and TRPO which&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:151px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;both use MPI for data passing. Both use Python3 and TensorFlow. PPO2 rep-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:163px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;resents a GPU-enabled implementation of PPO. This runs approximately 3X&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:175px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;faster than the current PPO baseline. In addition, baselines released an imple-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:187px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;mentation of Actor Critic with Experience Replay (ACER), a sample-efficient&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:199px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;policy gradient algorithm. ACER makes use of a replay buffer, enabling it to&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:211px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;perform more than one gradient update using each piece of sampled experience,&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:223px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;as well as a Q-Function approximate trained with the Retrace algorithm.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:235px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;The last algorithm that will be used is a Deep Q-Network also called DQN[16].&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:247px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;This algorithm is a variation of the classic Q-Learning algorithm. Markov De-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:259px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;cision Process (MDP)[19] is a sequential decision making process with Markov&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:271px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;property. It is represented as a tuple (&lt;/span&gt;&lt;span class=&quot;p8f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, &lt;/span&gt;&lt;span class=&quot;p8f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, T, R, γ&lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;). Markov property means&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:283px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;that the conditional probability distribution of the future state depends only on&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:295px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;the instant state and action instead of the entire state/action history, so it is&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:307px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;regarded as memoryless. In MDP setting, the system is fully observable which&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:319px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;means that the states can be derived from instant observations; i.e., &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p8f4&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;t &lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;= &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;f &lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p8f4&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;).&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:331px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Therefore, the agent can decide an action based on only instant observation &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p8f4&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;t&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:343px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;instead of what happened at previous times [11]. MDP consists of the following:&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:361px;&quot;&gt;&lt;span class=&quot;p8f5&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p8f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;State Space &lt;/span&gt;&lt;span class=&quot;p8f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;S &lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;- A set of all possible configurations of the system.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:381px;&quot;&gt;&lt;span class=&quot;p8f5&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p8f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Action Space &lt;/span&gt;&lt;span class=&quot;p8f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;A &lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;- A set of all possible actions of the agent.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:401px;&quot;&gt;&lt;span class=&quot;p8f5&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p8f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Model &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;T &lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;: &lt;/span&gt;&lt;span class=&quot;p8f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;S × S × A → &lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[0&lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, &lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;1] - A function of how environment evolves&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:410px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;through time, representing transition probabilities as &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;T &lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p8f7&quot; style=&quot;font-size:4px;vertical-align:super;color:rgba(0,0,0,1);&quot;&gt;′ &lt;/span&gt;&lt;span class=&quot;p8f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s, a&lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;) = &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p8f7&quot; style=&quot;font-size:4px;vertical-align:super;color:rgba(0,0,0,1);&quot;&gt;′ &lt;/span&gt;&lt;span class=&quot;p8f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s, a&lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;)&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:422px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;where &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p8f7&quot; style=&quot;font-size:4px;vertical-align:super;color:rgba(0,0,0,1);&quot;&gt;′ &lt;/span&gt;&lt;span class=&quot;p8f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;∈ S &lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is the next state, &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s &lt;/span&gt;&lt;span class=&quot;p8f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;∈ S &lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is the instant state and &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;a &lt;/span&gt;&lt;span class=&quot;p8f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;∈ A &lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:437px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;action taken.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:456px;&quot;&gt;&lt;span class=&quot;p8f5&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p8f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Reward Function &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;R &lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;: &lt;/span&gt;&lt;span class=&quot;p8f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;S × A → &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;R &lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;- A function of rewards obtained from&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:469px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;the environment. At each state transition &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p8f4&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;t &lt;/span&gt;&lt;span class=&quot;p8f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p8f4&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p8f8&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;+1&lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, a reward &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p8f4&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;t &lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is given&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:481px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;to the agent. Rewards may be either deterministic or stochastic. Reward&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:493px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;function is the expected value of reward given the state s and the action&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:505px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;taken a, defined by: &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s, a&lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;) = &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;E&lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p8f4&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p8f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p8f4&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;t &lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;= &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s, a&lt;/span&gt;&lt;span class=&quot;p8f4&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;t &lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;= &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;]&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:524px;&quot;&gt;&lt;span class=&quot;p8f5&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p8f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Discount Factor &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;γ &lt;/span&gt;&lt;span class=&quot;p8f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;∈ &lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[0&lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, &lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;1] - A measure of the importance of rewards in&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:536px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;the future for the value function.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:554px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;A key concept related to MDPs is the Q-function, &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p8f4&quot; style=&quot;font-size:6px;vertical-align:super;color:rgba(0,0,0,1);&quot;&gt;π &lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;: &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;S &lt;/span&gt;&lt;span class=&quot;p8f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;× &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;A &lt;/span&gt;&lt;span class=&quot;p8f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, that&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:568px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;defines the expected future discounted reward for taking action &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;a &lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;in state &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:580px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;and then following policy &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;π &lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;thereafter. According to the Bellman equation, the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:592px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Q-function for the optimal policy (denoted Q*) can be recursively expressed as:&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:185px; top:621px;&quot;&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p8f9&quot; style=&quot;font-size:6px;vertical-align:super;color:rgba(0,0,0,1);&quot;&gt;∗&lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s, a&lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;) = &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;T &lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s, a, s&lt;/span&gt;&lt;span class=&quot;p8f9&quot; style=&quot;font-size:6px;vertical-align:super;color:rgba(0,0,0,1);&quot;&gt;′&lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s, a, s&lt;/span&gt;&lt;span class=&quot;p8f9&quot; style=&quot;font-size:6px;vertical-align:super;color:rgba(0,0,0,1);&quot;&gt;′&lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;) + &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;γmax&lt;/span&gt;&lt;span class=&quot;p8f4&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p8f7&quot; style=&quot;font-size:4px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;′ &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p8f9&quot; style=&quot;font-size:6px;vertical-align:super;color:rgba(0,0,0,1);&quot;&gt;∗&lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p8f9&quot; style=&quot;font-size:6px;vertical-align:super;color:rgba(0,0,0,1);&quot;&gt;′&lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, a&lt;/span&gt;&lt;span class=&quot;p8f9&quot; style=&quot;font-size:6px;vertical-align:super;color:rgba(0,0,0,1);&quot;&gt;′&lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;)]&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:233px; top:636px;&quot;&gt;&lt;span class=&quot;p8f4&quot; style=&quot;font-size:6px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p8f7&quot; style=&quot;font-size:4px;vertical-align:super;color:rgba(0,0,0,1);&quot;&gt;′ &lt;/span&gt;&lt;span class=&quot;p8f9&quot; style=&quot;font-size:6px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;∈&lt;/span&gt;&lt;span class=&quot;p8f4&quot; style=&quot;font-size:6px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;S&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:653px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;where 0 &lt;/span&gt;&lt;span class=&quot;p8f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;≤ &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;γ &lt;/span&gt;&lt;span class=&quot;p8f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;≤ &lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;1 is the discount factor that defines how valuable near-term&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:665px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;rewards are compared to long-term rewards. Given Q&lt;/span&gt;&lt;span class=&quot;p8f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;∗&lt;/span&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, the optimal policy, &lt;/span&gt;&lt;span class=&quot;p8f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;p8f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;∗&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:303px; top:695px;&quot;&gt;&lt;span class=&quot;p8f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;8&lt;/span&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;



&lt;div id=&quot;page9&quot;&gt;

 
</Sec>

<Sec  gs2:docOID="HASHe01ede834a2ebcd3234133.1.9" gs2:mode="add">


&lt;div style=&quot;position: relative; height: 792px; width: 612px;&quot;&gt;&lt;style type=&quot;text/css&quot;&gt;

.txt { white-space:nowrap; }

.p9f0 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p9f1 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p9f2 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p9f3 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p9f4 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p9f5 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p9f6 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p9f7 { font-family:sans-serif; font-weight:normal; font-style:normal; }

&lt;/style&gt;

&lt;img class=&quot;background&quot; height=&quot;792&quot; id=&quot;background9&quot; src=&quot;_httpdocimg_/page9.png&quot; style=&quot;position:absolute; left:0px; top:0px;&quot; width=&quot;612&quot;&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:127px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, can be trivially recovered by greedily selecting the action in the current state&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:139px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;with the highest Q-value: &lt;/span&gt;&lt;span class=&quot;p9f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;p9f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;∗ &lt;/span&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;= &lt;/span&gt;&lt;span class=&quot;p9f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p9f4&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p9f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Q &lt;/span&gt;&lt;span class=&quot;p9f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;∗ &lt;/span&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;p9f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s, a&lt;/span&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;).This property has led to a&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:151px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;variety of learning algorithms that seek to directly estimate Q , and recover the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:163px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;optimal policy from it. Of particular note is Q-Learning.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:175px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Deep Q-Learning (DQN) makes 3 primary contributions to the classic Q-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:187px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Learning algorithm: (1) a deep convolutional neural net architecture for Q-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:199px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;function approximation; (2) using mini-batches of random training data rather&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:211px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;than single-step updates on the last experience; and (3) using older network&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:223px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;parameters to estimate the Q-values of the next state. The pseudocode for&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:235px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;DQN is shown on Figure 2. The deep convolutional architecture provides a&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:247px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;general purpose mechanism to estimate Q-function values from a short history&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:259px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;of image frames (in particular, the last 4 frames of experience). The latter two&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:271px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;contributions concern how to keep the iterative Q-function estimation stable.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:172px; top:494px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Figure 2: Pseudo code for Deep Q-Network (DQN) from [11]&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:520px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;DQN keeps a large history of the most recent experiences, where each expe-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:531px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;rience is a five-tuple (&lt;/span&gt;&lt;span class=&quot;p9f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s, a, s&lt;/span&gt;&lt;span class=&quot;p9f5&quot; style=&quot;font-size:6px;vertical-align:super;color:rgba(0,0,0,1);&quot;&gt;′&lt;/span&gt;&lt;span class=&quot;p9f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, r, T &lt;/span&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;), corresponding to an agent taking action &lt;/span&gt;&lt;span class=&quot;p9f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;a &lt;/span&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;in&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:542px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;state &lt;/span&gt;&lt;span class=&quot;p9f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, arriving in state &lt;/span&gt;&lt;span class=&quot;p9f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p9f5&quot; style=&quot;font-size:6px;vertical-align:super;color:rgba(0,0,0,1);&quot;&gt;′ &lt;/span&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;and receiving reward &lt;/span&gt;&lt;span class=&quot;p9f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;p9f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;T &lt;/span&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is a boolean indicating if&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:554px;&quot;&gt;&lt;span class=&quot;p9f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p9f5&quot; style=&quot;font-size:6px;vertical-align:super;color:rgba(0,0,0,1);&quot;&gt;′ &lt;/span&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is a terminal state. After each step in the environment, the agent adds the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:568px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;experience to its memory. After some small number of steps ([11] used 4), the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:580px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;agent randomly samples a mini-batch from its memory on which to perform its&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:592px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Q-function updates. Reusing previous experiences in updating a Q-function is&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:604px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;known as &lt;/span&gt;&lt;span class=&quot;p9f6&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;experience replay &lt;/span&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[10]. However, while experience replay in RL was&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:616px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;typically used to accelerate the backup of rewards, DQN’s approach of taking&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:628px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;fully random samples from its memory to use in mini-batch updates helps the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:640px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;samples from the environment that otherwise can cause bias in the function&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:652px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;approximation estimate.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:303px; top:695px;&quot;&gt;&lt;span class=&quot;p9f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;9&lt;/span&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;



&lt;div id=&quot;page10&quot;&gt;

 
</Sec>

<Sec  gs2:docOID="HASHe01ede834a2ebcd3234133.1.10" gs2:mode="add">


&lt;div style=&quot;position: relative; height: 792px; width: 612px;&quot;&gt;&lt;style type=&quot;text/css&quot;&gt;

.txt { white-space:nowrap; }

.p10f0 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p10f1 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p10f2 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p10f3 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p10f4 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p10f5 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p10f6 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p10f7 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p10f8 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p10f9 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p10f10 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p10f11 { font-family:sans-serif; font-weight:normal; font-style:normal; }

&lt;/style&gt;

&lt;img class=&quot;background&quot; height=&quot;792&quot; id=&quot;background10&quot; src=&quot;_httpdocimg_/page10.png&quot; style=&quot;position:absolute; left:0px; top:0px;&quot; width=&quot;612&quot;&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:124px;&quot;&gt;&lt;span class=&quot;p10f1&quot; style=&quot;font-size:14px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;4 Results&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:149px;&quot;&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;The trained ARS model in this paper uses seperate classes for Hyperparameters&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:161px;&quot;&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;and Normalization. Hyperparameters are variables that define how fast our&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:173px;&quot;&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;robot’s brain will work. These parameters require a lot of tuning in order to get&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:185px;&quot;&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;a good model that will enable the robot to learn efficently. In our case multiple&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:197px;&quot;&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;hyperparametras changes were done in order to optimize the outcome. The HP&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:209px;&quot;&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;that were selected as best for this training are:&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:228px;&quot;&gt;&lt;span class=&quot;p10f3&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;number of steps: 1000&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:248px;&quot;&gt;&lt;span class=&quot;p10f3&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;episode length: 2000&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:268px;&quot;&gt;&lt;span class=&quot;p10f3&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;learning rate: 0.2&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:288px;&quot;&gt;&lt;span class=&quot;p10f3&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;number of deltas: 16&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:308px;&quot;&gt;&lt;span class=&quot;p10f3&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;number of best deltas: 16&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:328px;&quot;&gt;&lt;span class=&quot;p10f3&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;noise: 0.03&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:348px;&quot;&gt;&lt;span class=&quot;p10f3&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;seed: 1&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:368px;&quot;&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;An uncertainty in the agent movements in this algorithm is present due to the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:380px;&quot;&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;random search for actions. For that particular reason, we want to normalize all&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:392px;&quot;&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;of the robot’s behaviours (the size of its step, speed of walking, etc.) so that we&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:404px;&quot;&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;can directly compare parameters with different units. To do this, we converted&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:416px;&quot;&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;the robot’s movement parameters to z-scores using the known formula:&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:187px; top:448px;&quot;&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;p10f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;currentposition &lt;/span&gt;&lt;span class=&quot;p10f5&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;− &lt;/span&gt;&lt;span class=&quot;p10f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;meanposition&lt;/span&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;p10f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;/standarddeviation&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:470px;&quot;&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;After training the model, a policy evaluation is required in order to see how&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:482px;&quot;&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;much did the agent learn and to update the behavioural model accordingly. The&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:494px;&quot;&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;update is done using the following formula:&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:271px; top:516px;&quot;&gt;&lt;span class=&quot;p10f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;θ&lt;/span&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;+&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:287px; top:516px;&quot;&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;=&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:316px; top:514px;&quot;&gt;&lt;span class=&quot;p10f6&quot; style=&quot;font-size:6px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;α&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:299px; top:521px;&quot;&gt;&lt;span class=&quot;p10f7&quot; style=&quot;font-size:6px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;p10f6&quot; style=&quot;font-size:6px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;δ&lt;/span&gt;&lt;span class=&quot;p10f8&quot; style=&quot;font-size:4px;vertical-align:super;color:rgba(0,0,0,1);&quot;&gt;∗&lt;/span&gt;&lt;span class=&quot;p10f9&quot; style=&quot;font-size:6px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;×&lt;/span&gt;&lt;span class=&quot;p10f6&quot; style=&quot;font-size:6px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;σ&lt;/span&gt;&lt;span class=&quot;p10f10&quot; style=&quot;font-size:4px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;r &lt;/span&gt;&lt;span class=&quot;p10f7&quot; style=&quot;font-size:6px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;p10f9&quot; style=&quot;font-size:6px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;×&lt;/span&gt;&lt;span class=&quot;p10f6&quot; style=&quot;font-size:6px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:539px;&quot;&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Each of the arguments are explained bellow:&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:558px;&quot;&gt;&lt;span class=&quot;p10f3&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p10f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;θ &lt;/span&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is the weight assigned to a particular behaviour. From a neuroscience&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:571px;&quot;&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;perspective, it is something like a long-term potentiation or inhibition&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:583px;&quot;&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(strengthening or weakening) of a neural connection. If the behaviour is&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:595px;&quot;&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;beneficial, then we want to strengthen the neurons driving that behaviour&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:607px;&quot;&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(and vice versa).&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:626px;&quot;&gt;&lt;span class=&quot;p10f3&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p10f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;alpha &lt;/span&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is the learning rate, the speed at which the model learns. If the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:639px;&quot;&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;model learns too fast, it may overshoot the optimal setup for making a&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:650px;&quot;&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;good decision (i.e. making too large of a step and falling over).&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:300px; top:695px;&quot;&gt;&lt;span class=&quot;p10f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;10&lt;/span&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;



&lt;div id=&quot;page11&quot;&gt;

 
</Sec>

<Sec  gs2:docOID="HASHe01ede834a2ebcd3234133.2" gs2:mode="add">


 
</Sec>

<Sec  gs2:docOID="HASHe01ede834a2ebcd3234133.2.1" gs2:mode="add">


&lt;div style=&quot;position: relative; height: 792px; width: 612px;&quot;&gt;&lt;style type=&quot;text/css&quot;&gt;

.txt { white-space:nowrap; }

.p11f0 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p11f1 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p11f2 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p11f3 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p11f4 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p11f5 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p11f6 { font-family:sans-serif; font-weight:normal; font-style:normal; }

&lt;/style&gt;

&lt;img class=&quot;background&quot; height=&quot;792&quot; id=&quot;background11&quot; src=&quot;_httpdocimg_/page11.png&quot; style=&quot;position:absolute; left:0px; top:0px;&quot; width=&quot;612&quot;&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:126px;&quot;&gt;&lt;span class=&quot;p11f1&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p11f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;δ&lt;/span&gt;&lt;span class=&quot;p11f3&quot; style=&quot;font-size:6px;vertical-align:super;color:rgba(0,0,0,1);&quot;&gt;∗ &lt;/span&gt;&lt;span class=&quot;p11f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;represents the robot’s past experiences. Here, the robot is ’thinking’&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:139px;&quot;&gt;&lt;span class=&quot;p11f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;about the decisions it made in the past, and the optimal deltas - the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:151px;&quot;&gt;&lt;span class=&quot;p11f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;optimal shifts in strategy it has made.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:171px;&quot;&gt;&lt;span class=&quot;p11f1&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p11f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;σ&lt;/span&gt;&lt;span class=&quot;p11f5&quot; style=&quot;font-size:6px;vertical-align:sub;color:rgba(0,0,0,1);&quot;&gt;r &lt;/span&gt;&lt;span class=&quot;p11f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is the sum of the rewards the robot has gained for making specific&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:183px;&quot;&gt;&lt;span class=&quot;p11f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;behaviours of that type before. If the action resulted in the robot making&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:195px;&quot;&gt;&lt;span class=&quot;p11f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;progress towards the goal in the past, it’s assigned a positive reward. If the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:207px;&quot;&gt;&lt;span class=&quot;p11f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;action resulted in the robot falling, it’s assigned a punishment (negative&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:158px; top:219px;&quot;&gt;&lt;span class=&quot;p11f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;reward).&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:238px;&quot;&gt;&lt;span class=&quot;p11f1&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p11f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;s &lt;/span&gt;&lt;span class=&quot;p11f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;represents the step that the agent has made in that moment&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:259px;&quot;&gt;&lt;span class=&quot;p11f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Our agent is further represented by a class ARSTrainer which uses the above&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:271px;&quot;&gt;&lt;span class=&quot;p11f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;mentioned hyper parameters, normalizer and policy. This class has two meth-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:283px;&quot;&gt;&lt;span class=&quot;p11f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;ods: explore and train. The explore function is called after each episode of&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:295px;&quot;&gt;&lt;span class=&quot;p11f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;training. Here we’re telling the robot to go forth into this brave new world, try&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:307px;&quot;&gt;&lt;span class=&quot;p11f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;out various strategies according to its policies, and note down the rewards and&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:319px;&quot;&gt;&lt;span class=&quot;p11f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;punishments it gets. The train function consists of the actual learning for the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:331px;&quot;&gt;&lt;span class=&quot;p11f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;agent. It uses the rewards and punishments that the agent has faced during&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:343px;&quot;&gt;&lt;span class=&quot;p11f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;exploration to update its behaviour for the next strategy that it will try.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:192px; top:654px;&quot;&gt;&lt;span class=&quot;p11f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Figure 3: Rewards per episode with the ARS model&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:300px; top:695px;&quot;&gt;&lt;span class=&quot;p11f4&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;11&lt;/span&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;



&lt;div id=&quot;page12&quot;&gt;

 
</Sec>

<Sec  gs2:docOID="HASHe01ede834a2ebcd3234133.2.2" gs2:mode="add">


&lt;div style=&quot;position: relative; height: 792px; width: 612px;&quot;&gt;&lt;style type=&quot;text/css&quot;&gt;

.txt { white-space:nowrap; }

.p12f0 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p12f1 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p12f2 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p12f3 { font-family:sans-serif; font-weight:normal; font-style:normal; }

&lt;/style&gt;

&lt;img class=&quot;background&quot; height=&quot;792&quot; id=&quot;background12&quot; src=&quot;_httpdocimg_/page12.png&quot; style=&quot;position:absolute; left:0px; top:0px;&quot; width=&quot;612&quot;&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:127px;&quot;&gt;&lt;span class=&quot;p12f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Figure 3 represents the rewards the agent has gathered during a training&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:139px;&quot;&gt;&lt;span class=&quot;p12f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;session of 2000 episodes with number of steps set to 200.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:151px;&quot;&gt;&lt;span class=&quot;p12f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;From the figure, it can be concluded that the agent using ARSTrainer is&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:163px;&quot;&gt;&lt;span class=&quot;p12f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;learning arbitrarily. The rewards are moving on approximately the same range&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:175px;&quot;&gt;&lt;span class=&quot;p12f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;of rewards between [5, -40] in the episodes until reaching 5000. After that, an&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:187px;&quot;&gt;&lt;span class=&quot;p12f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;drastic improvement is detected showing that the agent got reward bigger than&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:199px;&quot;&gt;&lt;span class=&quot;p12f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;20. The agent trained with ARS in our paper used 1000 number of steps and&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:211px;&quot;&gt;&lt;span class=&quot;p12f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;2000 number of episodes. It resulted in having the agent walk with no failing in&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:223px;&quot;&gt;&lt;span class=&quot;p12f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;the last episode, but having the agent fail before reaching the end in the episode&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:235px;&quot;&gt;&lt;span class=&quot;p12f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;before that.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:247px;&quot;&gt;&lt;span class=&quot;p12f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;The second trained model is DQN model. The architecture of the model used&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:259px;&quot;&gt;&lt;span class=&quot;p12f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;is almost identical to the architecture we used in the exercises, with several new&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:271px;&quot;&gt;&lt;span class=&quot;p12f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;features. The HP that were selected as best for this training are:&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:290px;&quot;&gt;&lt;span class=&quot;p12f2&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p12f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;episode length: 200&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:310px;&quot;&gt;&lt;span class=&quot;p12f2&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p12f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;learning rate: 0.01&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:330px;&quot;&gt;&lt;span class=&quot;p12f2&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p12f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;gamma: 0.98&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:351px;&quot;&gt;&lt;span class=&quot;p12f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;The network used for this algorithm is simple Neural Network with 2 hidden&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:363px;&quot;&gt;&lt;span class=&quot;p12f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;layers:&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:382px;&quot;&gt;&lt;span class=&quot;p12f2&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p12f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;model.add(Dense(400, inputdim=self.nx, activation=’relu’))&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:402px;&quot;&gt;&lt;span class=&quot;p12f2&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p12f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;model.add(Dense(300, activation=’relu’))&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:422px;&quot;&gt;&lt;span class=&quot;p12f2&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p12f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;model.add(Dense(self.ny, activation=’linear’))&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:442px;&quot;&gt;&lt;span class=&quot;p12f2&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p12f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;model.compile(loss=’mse’, optimizer=Adam(lr=self.lr))&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:462px;&quot;&gt;&lt;span class=&quot;p12f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;The results from the rewards are shown in Figure 4.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:300px; top:695px;&quot;&gt;&lt;span class=&quot;p12f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;12&lt;/span&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;



&lt;div id=&quot;page13&quot;&gt;

 
</Sec>

<Sec  gs2:docOID="HASHe01ede834a2ebcd3234133.2.3" gs2:mode="add">


&lt;div style=&quot;position: relative; height: 792px; width: 612px;&quot;&gt;&lt;style type=&quot;text/css&quot;&gt;

.txt { white-space:nowrap; }

.p13f0 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p13f1 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p13f2 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p13f3 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p13f4 { font-family:sans-serif; font-weight:normal; font-style:normal; }

&lt;/style&gt;

&lt;img class=&quot;background&quot; height=&quot;792&quot; id=&quot;background13&quot; src=&quot;_httpdocimg_/page13.png&quot; style=&quot;position:absolute; left:0px; top:0px;&quot; width=&quot;612&quot;&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:191px; top:352px;&quot;&gt;&lt;span class=&quot;p13f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Figure 4: Rewards per episode with the DQN model&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:378px;&quot;&gt;&lt;span class=&quot;p13f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;From the results it can be concluded that DQN, trained with the given&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:390px;&quot;&gt;&lt;span class=&quot;p13f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;parameters and neural network, is not the best choice. The reason for these&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:402px;&quot;&gt;&lt;span class=&quot;p13f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;results is that DQN needs to be trained with a more complex network, multiple&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:414px;&quot;&gt;&lt;span class=&quot;p13f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;episodes and trained hyper parameters, but at the moment we are not able to&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:426px;&quot;&gt;&lt;span class=&quot;p13f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;because we are limited due to the processing power of computers. However, it is&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:438px;&quot;&gt;&lt;span class=&quot;p13f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;worth mentioning that DQN is not guaranteed to converge because of the insta-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:450px;&quot;&gt;&lt;span class=&quot;p13f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;bility caused by bootstrapping, sampling and value functional approximation.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:462px;&quot;&gt;&lt;span class=&quot;p13f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;In order for the policy to converge, we need to ensure the policy is near greedy&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:474px;&quot;&gt;&lt;span class=&quot;p13f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;(epsilon is close to 0) once the value is converge, which could be achieved by&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:485px;&quot;&gt;&lt;span class=&quot;p13f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;playing a large enough number of episodes.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:497px;&quot;&gt;&lt;span class=&quot;p13f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;The third trained model is PPO model. Actor Critic approach was used&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:509px;&quot;&gt;&lt;span class=&quot;p13f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;for our PPO agent. It uses two models, both Deep Neural Nets, one called the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:521px;&quot;&gt;&lt;span class=&quot;p13f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Actor and other called the Critic. The architecture is shown in Figure 5.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:300px; top:695px;&quot;&gt;&lt;span class=&quot;p13f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;13&lt;/span&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;



&lt;div id=&quot;page14&quot;&gt;

 
</Sec>

<Sec  gs2:docOID="HASHe01ede834a2ebcd3234133.2.4" gs2:mode="add">


&lt;div style=&quot;position: relative; height: 792px; width: 612px;&quot;&gt;&lt;style type=&quot;text/css&quot;&gt;

.txt { white-space:nowrap; }

.p14f0 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p14f1 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p14f2 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p14f3 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p14f4 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p14f5 { font-family:sans-serif; font-weight:normal; font-style:normal; }

&lt;/style&gt;

&lt;img class=&quot;background&quot; height=&quot;792&quot; id=&quot;background14&quot; src=&quot;_httpdocimg_/page14.png&quot; style=&quot;position:absolute; left:0px; top:0px;&quot; width=&quot;612&quot;&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:230px; top:294px;&quot;&gt;&lt;span class=&quot;p14f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Figure 5: PPO model architecture&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:320px;&quot;&gt;&lt;span class=&quot;p14f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;The key contribution of PPO is ensuring that a new update of the policy does&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:332px;&quot;&gt;&lt;span class=&quot;p14f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;not change it too much from the previous policy. This leads to less variance in&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:344px;&quot;&gt;&lt;span class=&quot;p14f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;training at the cost of some bias, but ensures smoother training and also makes&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:356px;&quot;&gt;&lt;span class=&quot;p14f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;sure the agent does not go down an unrecoverable path of taking senseless&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:368px;&quot;&gt;&lt;span class=&quot;p14f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;actions.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:380px;&quot;&gt;&lt;span class=&quot;p14f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;The HP that were selected as best for this training are:&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:399px;&quot;&gt;&lt;span class=&quot;p14f4&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p14f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;episode length: 200&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:419px;&quot;&gt;&lt;span class=&quot;p14f4&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p14f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;learning rate: 0.0001&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:439px;&quot;&gt;&lt;span class=&quot;p14f4&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p14f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;gamma: 0.99&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:459px;&quot;&gt;&lt;span class=&quot;p14f4&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p14f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;alpha: 0.1&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:479px;&quot;&gt;&lt;span class=&quot;p14f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;The Actor was trained with a Neural Network with 2 hidden layers using&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:491px;&quot;&gt;&lt;span class=&quot;p14f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;RELU as an activation function, and the output layer is using tanh. The used&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:503px;&quot;&gt;&lt;span class=&quot;p14f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;optimizer is Adam. The Critic is using the same hidden layers, and his output&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:515px;&quot;&gt;&lt;span class=&quot;p14f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;layer is trained with linear activation function.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:527px;&quot;&gt;&lt;span class=&quot;p14f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;The rewards from PPO model are shown in figure 6.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:300px; top:695px;&quot;&gt;&lt;span class=&quot;p14f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;14&lt;/span&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;



&lt;div id=&quot;page15&quot;&gt;

 
</Sec>

<Sec  gs2:docOID="HASHe01ede834a2ebcd3234133.2.5" gs2:mode="add">


&lt;div style=&quot;position: relative; height: 792px; width: 612px;&quot;&gt;&lt;style type=&quot;text/css&quot;&gt;

.txt { white-space:nowrap; }

.p15f0 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p15f1 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p15f2 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p15f3 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p15f4 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p15f5 { font-family:sans-serif; font-weight:normal; font-style:normal; }

&lt;/style&gt;

&lt;img class=&quot;background&quot; height=&quot;792&quot; id=&quot;background15&quot; src=&quot;_httpdocimg_/page15.png&quot; style=&quot;position:absolute; left:0px; top:0px;&quot; width=&quot;612&quot;&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:191px; top:366px;&quot;&gt;&lt;span class=&quot;p15f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Figure 6: Rewards per episode with the PPO model&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:392px;&quot;&gt;&lt;span class=&quot;p15f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;After PPO, we trained PPO2 model from baselines, using MlpPolicy. Stable-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:404px;&quot;&gt;&lt;span class=&quot;p15f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;baselines provides a set of default policies, that can be used with most action&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:416px;&quot;&gt;&lt;span class=&quot;p15f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;spaces. MLP Policy implements actor critic, using a Multi-layer Perceptron (2&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:428px;&quot;&gt;&lt;span class=&quot;p15f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;layers of 64).&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:440px;&quot;&gt;&lt;span class=&quot;p15f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;The HP that were selected as best for this training are:&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:459px;&quot;&gt;&lt;span class=&quot;p15f4&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p15f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;episode length: 100&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:479px;&quot;&gt;&lt;span class=&quot;p15f4&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p15f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;learning rate: 3e-4&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:499px;&quot;&gt;&lt;span class=&quot;p15f4&quot; style=&quot;font-size:5px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;• &lt;/span&gt;&lt;span class=&quot;p15f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;gamma: 0.99&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:520px;&quot;&gt;&lt;span class=&quot;p15f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;The model was trained with 4000 total timestamps. The results from PPO2&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:532px;&quot;&gt;&lt;span class=&quot;p15f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;are shown on Figure 7.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:300px; top:695px;&quot;&gt;&lt;span class=&quot;p15f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;15&lt;/span&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;



&lt;div id=&quot;page16&quot;&gt;

 
</Sec>

<Sec  gs2:docOID="HASHe01ede834a2ebcd3234133.2.6" gs2:mode="add">


&lt;div style=&quot;position: relative; height: 792px; width: 612px;&quot;&gt;&lt;style type=&quot;text/css&quot;&gt;

.txt { white-space:nowrap; }

.p16f0 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p16f1 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p16f2 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p16f3 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p16f4 { font-family:sans-serif; font-weight:normal; font-style:normal; }

&lt;/style&gt;

&lt;img class=&quot;background&quot; height=&quot;792&quot; id=&quot;background16&quot; src=&quot;_httpdocimg_/page16.png&quot; style=&quot;position:absolute; left:0px; top:0px;&quot; width=&quot;612&quot;&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:189px; top:351px;&quot;&gt;&lt;span class=&quot;p16f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Figure 7: Rewards per episode with the PPO2 model&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:377px;&quot;&gt;&lt;span class=&quot;p16f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;The last thing that we want to share from results is the hard-coded model.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:389px;&quot;&gt;&lt;span class=&quot;p16f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;This model is specially built for this environment and that is the obvious reason&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:401px;&quot;&gt;&lt;span class=&quot;p16f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;why this model has the best results. Figure 8 shows the rewards that were&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:413px;&quot;&gt;&lt;span class=&quot;p16f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;gathered per each episode training this hard-coded model.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:178px; top:662px;&quot;&gt;&lt;span class=&quot;p16f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Figure 8: Rewards per episode with the hard-coded model&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:300px; top:695px;&quot;&gt;&lt;span class=&quot;p16f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;16&lt;/span&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;



&lt;div id=&quot;page17&quot;&gt;

 
</Sec>

<Sec  gs2:docOID="HASHe01ede834a2ebcd3234133.2.7" gs2:mode="add">


&lt;div style=&quot;position: relative; height: 792px; width: 612px;&quot;&gt;&lt;style type=&quot;text/css&quot;&gt;

.txt { white-space:nowrap; }

.p17f0 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p17f1 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p17f2 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p17f3 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p17f4 { font-family:sans-serif; font-weight:normal; font-style:normal; }

&lt;/style&gt;

&lt;img class=&quot;background&quot; height=&quot;792&quot; id=&quot;background17&quot; src=&quot;_httpdocimg_/page17.png&quot; style=&quot;position:absolute; left:0px; top:0px;&quot; width=&quot;612&quot;&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:124px;&quot;&gt;&lt;span class=&quot;p17f1&quot; style=&quot;font-size:14px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;5 Conclusion&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:149px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;With the results examined from all methods there are conclusions that can be&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:161px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;made for each one. The ARS method for training an agent to walk is making the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:173px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;agent learn at random and not that much from past experience. That results&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:185px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;in having the curve of rewards going up and down and not being sure if the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:197px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;agent is learning or not. This agent should be trained using this RL method on&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:209px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;a bigger number set for the number of episodes, as well as for the number of&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:221px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;steps.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:233px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;From the results, the PPO and PPO2 method are not showing us a significant&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:245px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;reward curve from which we can conclude if the agent is learning or not. It gives&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:257px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;us uncertainty of the outcome because the rewards curve is going well and in&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:269px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;one particular moment it is making a significant mistake. This means that a&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:281px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;long hyper-parameter tuning process is required in order to find when does the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:293px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;agent make this mistake and how he should overcome it.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:305px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Although these models were not trained on big number of episodes, it can&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:317px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;be concluded that the agent trained using the DQN algorithm is showing re-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:329px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;markable and consistent increasing of the reward curve compared to others. On&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:341px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;the other hand, the variance of this model is certainly showing a varying curve&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:353px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;which is making this algorithm uncertain that it will learn better each time we&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:364px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;start learning.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:376px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;All of the methods are acting particularly exceptional for this environment&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:388px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;and should be truly examined in future works with longer episodes numbers.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:400px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;What can be of significant importance is that the last episodes of each method&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:412px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;gave high rewards of the agent walking which is making us believe that by the&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:424px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;end of each method tried the agent has learned to walk.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:148px; top:436px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;There might be a few possible ways to improve this work. First of all,&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:448px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;longer observation history can be used to handle partial observably for LSTM&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:460px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;and Transformer models. However, this makes learning slower and difficult and&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:472px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;requires more stable RL algorithms and fine tuning on hyper-parameters. Sec-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:484px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;ondly, different exploration strategies might be followed. Especially parameter&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:496px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;space exploration may perform better since it works better for environments&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:508px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;with sparse reward like BipedalWalker. Lastly, advanced neural networks and&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:520px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;RL algorithms (specifically designed for environment) may be designed by in-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:532px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;corporating domain knowledge.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:562px;&quot;&gt;&lt;span class=&quot;p17f1&quot; style=&quot;font-size:14px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;References&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:138px; top:587px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[1] Neda Ahmadi, Diego Cabo, Sudhakaran Jain, and Ruben Kip. Comparing&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:599px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;continuous reinforcement learning algorithms using openai.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:138px; top:618px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[2] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:630px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. &lt;/span&gt;&lt;span class=&quot;p17f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;arXiv preprint&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:642px;&quot;&gt;&lt;span class=&quot;p17f3&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;arXiv:1606.01540&lt;/span&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, 2016.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:300px; top:695px;&quot;&gt;&lt;span class=&quot;p17f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;17&lt;/span&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;



&lt;div id=&quot;page18&quot;&gt;

 
</Sec>

<Sec  gs2:docOID="HASHe01ede834a2ebcd3234133.2.8" gs2:mode="add">


&lt;div style=&quot;position: relative; height: 792px; width: 612px;&quot;&gt;&lt;style type=&quot;text/css&quot;&gt;

.txt { white-space:nowrap; }

.p18f0 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p18f1 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p18f2 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p18f3 { font-family:monospace; font-weight:normal; font-style:normal; }

.p18f4 { font-family:sans-serif; font-weight:normal; font-style:normal; }

&lt;/style&gt;

&lt;img class=&quot;background&quot; height=&quot;792&quot; id=&quot;background18&quot; src=&quot;_httpdocimg_/page18.png&quot; style=&quot;position:absolute; left:0px; top:0px;&quot; width=&quot;612&quot;&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:138px; top:127px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[3] Erin Catto. Box2d: A 2d physics engine for games. &lt;/span&gt;&lt;span class=&quot;p18f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;URL: http://www.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:139px;&quot;&gt;&lt;span class=&quot;p18f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;box2d. org&lt;/span&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, 2011.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:138px; top:159px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[4] Andr´ea Deslandes, Helena Moraes, Camila Ferreira, Heloisa Veiga, Heitor&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:171px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Silveira, Raphael Mouta, Fernando AMS Pompeu, Evandro Silva Freire&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:183px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Coutinho, and Jerson Laks. Exercise and mental health: many reasons to&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:195px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;move. &lt;/span&gt;&lt;span class=&quot;p18f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Neuropsychobiology&lt;/span&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, 59(4):191–198, 2009.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:138px; top:215px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[5] David Ha. Reinforcement learning for improving agent design. &lt;/span&gt;&lt;span class=&quot;p18f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Artificial&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:227px;&quot;&gt;&lt;span class=&quot;p18f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;life&lt;/span&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, 25(4):352–365, 2019.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:138px; top:247px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[6] Ashley Hill, Antonin Raffin, Maximilian Ernestus, Adam Gleave, Anssi&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:259px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Kanervisto, Rene Traore, Prafulla Dhariwal, Christopher Hesse, Oleg&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:271px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman,&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:283px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Szymon Sidor, and Yuhuai Wu. Stable baselines. &lt;/span&gt;&lt;span class=&quot;p18f3&quot; style=&quot;font-size:8px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;https://github.com/&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:295px;&quot;&gt;&lt;span class=&quot;p18f3&quot; style=&quot;font-size:8px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;hill-a/stable-baselines&lt;/span&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, 2018.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:138px; top:315px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[7] Mohammed Hossny, Julie Iskander, Mohamed Attia, Khaled Saleh,&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:327px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;and Ahmed Abobakr. Refined continuous control of ddpg actors via&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:339px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;parametrised activation. &lt;/span&gt;&lt;span class=&quot;p18f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;AI&lt;/span&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, 2(4):464–476, 2021.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:138px; top:359px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[8] Jonathan Hui. RL — DQN Deep Q-network.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:138px; top:379px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[9] Filip Wolski Prafulla Dhariwal Alec Radford John Schulman, Oleg Klimov.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:390px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Proximal Policy Optimization.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:410px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[10] Long-Ji Lin. Self-improving reactive agents based on reinforcement learn-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:422px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;ing, planning and teaching. &lt;/span&gt;&lt;span class=&quot;p18f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Machine learning&lt;/span&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, 8(3-4):293–321, 1992.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:442px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[11] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:454px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:466px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Fidjeland, Georg Ostrovski, et al. Human-level control through deep rein-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:478px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;forcement learning. &lt;/span&gt;&lt;span class=&quot;p18f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;nature&lt;/span&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, 518(7540):529–533, 2015.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:498px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[12] OpenAI. BipedalWalker-v2 enviornment OpenAI Gym.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:515px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[13] Ug˘urcan O¨ zalp. Bipedal robot walking by reinforcement learning in par-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:530px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;tially observed environment. Master’s thesis, Middle East Technical Uni-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:542px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;versity, 2021.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:562px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[14] Allen S Parseghian. &lt;/span&gt;&lt;span class=&quot;p18f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Control of a simulated, three-dimensional bipedal robot&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:574px;&quot;&gt;&lt;span class=&quot;p18f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;to initiate walking, continue walking, rock side-to-side, and balance&lt;/span&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;. PhD&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:586px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;thesis, Massachusetts Institute of Technology, 2000.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:606px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[15] Divyam Rastogi. Deep reinforcement learning for bipedal robots. 2017.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:626px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[16] Melrose Roderick, James MacGlashan, and Stefanie Tellex. Implementing&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:638px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;the deep q-network. &lt;/span&gt;&lt;span class=&quot;p18f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;arXiv preprint arXiv:1711.07478&lt;/span&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, 2017.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:657px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[17] Ziad SALLOUM. Introduction to Augmented Random Search.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:300px; top:695px;&quot;&gt;&lt;span class=&quot;p18f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;18&lt;/span&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;



&lt;div id=&quot;page19&quot;&gt;

 
</Sec>

<Sec  gs2:docOID="HASHe01ede834a2ebcd3234133.2.9" gs2:mode="add">


&lt;div style=&quot;position: relative; height: 792px; width: 612px;&quot;&gt;&lt;style type=&quot;text/css&quot;&gt;

.txt { white-space:nowrap; }

.p19f0 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p19f1 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p19f2 { font-family:sans-serif; font-weight:normal; font-style:normal; }

.p19f3 { font-family:sans-serif; font-weight:normal; font-style:normal; }

&lt;/style&gt;

&lt;img class=&quot;background&quot; height=&quot;792&quot; id=&quot;background19&quot; src=&quot;_httpdocimg_/page19.png&quot; style=&quot;position:absolute; left:0px; top:0px;&quot; width=&quot;612&quot;&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:127px;&quot;&gt;&lt;span class=&quot;p19f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[18] Hemant Singh. Deep Deterministic Policy Gradient (DDPG).&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:147px;&quot;&gt;&lt;span class=&quot;p19f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[19] Wikipedia source. Markov decision process.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:167px;&quot;&gt;&lt;span class=&quot;p19f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[20] Valmor Tricoli, Leonardo Lamas, Roberto Carnevale, and Carlos Ugri-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:179px;&quot;&gt;&lt;span class=&quot;p19f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;nowitsch. Short-term effects on lower-body functional power development:&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:191px;&quot;&gt;&lt;span class=&quot;p19f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;weightlifting vs. vertical jump training programs. &lt;/span&gt;&lt;span class=&quot;p19f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;The Journal of Strength&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:203px;&quot;&gt;&lt;span class=&quot;p19f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;&amp;amp; Conditioning Research&lt;/span&gt;&lt;span class=&quot;p19f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, 19(2):433–437, 2005.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:223px;&quot;&gt;&lt;span class=&quot;p19f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[21] Chang Wang, Chao Yan, Xiaojia Xiang, and Han Zhou. A continuous actor-&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:235px;&quot;&gt;&lt;span class=&quot;p19f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;critic reinforcement learning approach to flocking with fixed-wing uavs.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:247px;&quot;&gt;&lt;span class=&quot;p19f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;101:64–79, 17–19 Nov 2019.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:267px;&quot;&gt;&lt;span class=&quot;p19f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;[22] Anton Orell Wiehe, Nil Stolt Ans´o, Madalina M Drugan, and Marco A&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:279px;&quot;&gt;&lt;span class=&quot;p19f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Wiering. Sampled policy gradient for learning to play the game agar. io.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:154px; top:291px;&quot;&gt;&lt;span class=&quot;p19f2&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;arXiv preprint arXiv:1809.05763&lt;/span&gt;&lt;span class=&quot;p19f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;, 2018.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:133px; top:313px;&quot;&gt;&lt;span class=&quot;p19f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;Here you can see the Github repo.&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;txt&quot; style=&quot;position:absolute; left:300px; top:695px;&quot;&gt;&lt;span class=&quot;p19f1&quot; style=&quot;font-size:9px;vertical-align:baseline;color:rgba(0,0,0,1);&quot;&gt;19&lt;/span&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;



 
</Sec>


</Doc>
